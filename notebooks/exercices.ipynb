{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pr√©liminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/Zakaria-Yahya/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88110170-8b7d-454e-9387-d0dd7e11006f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'exam_2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie th√©orique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit √™tre personnel). Cet entier servira de graine au g√©n√©rateur de nombres al√©atoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 388"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation lin√©aire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de donn√©es (entra√Ænement et test). Pour chaque jeu de donn√©es, la clef 'inputs' donne acc√®s √† un tableau numpy (numpy array) de pr√©dicteurs empil√©s horizontalement : chaque ligne $i$ contient trois pr√©dicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont li√©es aux pr√©dicteurs par le mod√®le:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ o√π $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle m√©thode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R√©ponse:\n",
        "La m√©thode la plus simple et la plus efficace pour estimer les coefficients (Œ∏) dans un mod√®le de r√©gression lin√©aire comme celui-ci est d'utiliser les moindres carr√©s ordinaires (OLS)."
      ],
      "metadata": {
        "id": "gDgh0UzhZU6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraire les entr√©es (X) et les cibles (y) de l'ensemble d'apprentissage\n",
        "X = torch.tensor(train_set['inputs'], dtype=torch.float32)\n",
        "y = torch.tensor(train_set['targets'], dtype=torch.float32)\n",
        "\n",
        "# Ajouter une colonne de uns √† X pour le terme d'interception (Œ∏0)\n",
        "X = torch.cat([torch.ones(X.shape[0], 1), X], dim=1)\n",
        "\n",
        "# Calculer les coefficients OLS √† l'aide de la formule : Œ∏ = (X^T X)^-1 X^T y\n",
        "theta = torch.linalg.solve(X.T @ X, X.T @ y)\n",
        "\n",
        "# Afficher les coefficients estim√©s\n",
        "print(\"Coefficients estim√©s (Œ∏) :\", theta)\n",
        "\n",
        "# Calculate predictions\n",
        "predictions = X @ theta\n",
        "\n",
        "# Calculate error (Mean Squared Error - MSE)\n",
        "error = torch.mean((predictions - y)**2)\n",
        "\n",
        "# Print the error\n",
        "print(\"Mean Squared Error (MSE):\", error)"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df10d0c-b477-442b-8a6e-5237514e3d8b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients estim√©s (Œ∏) : tensor([19.3857,  3.9040,  3.8649,  7.5905])\n",
            "Mean Squared Error (MSE): tensor(3.9825)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ gr√¢ce √† un r√©seau de neurones entra√Æn√© par SGD. Quelle architecture s'y pr√™te ? Justifier en termes d'expressivit√© et de performances en g√©n√©ralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R√©ponse:\n",
        "\n",
        "Expressivit√© : Une seule couche lin√©aire est suffisante pour apprendre une relation lin√©aire. Ajouter des couches suppl√©mentaires ou des non-lin√©arit√©s serait superflu et risquerait de d√©grader les performances en g√©n√©ralisation.\n",
        "\n",
        "Performances en g√©n√©ralisation : Une architecture simple r√©duit le risque de sur-apprentissage (overfitting), surtout avec un bruit gaussien (œµ‚àºN(0,Œ∑)). Cela garantit une bonne performance sur des donn√©es non vues."
      ],
      "metadata": {
        "id": "qFss527GcByM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "import torch.nn as nn\n",
        "\n",
        "# A coder :\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc = nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entra√Æner cette architecture √† la t√¢che de r√©gression d√©finie par les entr√©es et sorties du jeu d'entra√Ænement (compl√©ter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "\n",
        "        # Mise en mode entra√Ænement\n",
        "        mySimpleNet.train()\n",
        "\n",
        "        # √âtape forward : calcul des pr√©dictions\n",
        "        predictions = mySimpleNet(batch_inputs)\n",
        "\n",
        "        # Calcul de la perte\n",
        "        loss = criterion(predictions, batch_targets.unsqueeze(1))  # Ajouter une dimension pour le batch\n",
        "\n",
        "        # √âtape backward : calcul des gradients\n",
        "        optimizer.zero_grad()  # Remise √† z√©ro des gradients\n",
        "        loss.backward()  # Calcul des gradients\n",
        "\n",
        "        # Mise √† jour des param√®tres\n",
        "        optimizer.step()\n",
        "\n",
        "    # Affichage de la perte pour suivre l'entra√Ænement\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        ""
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59820346-574b-4dc9-bf8f-308c8ac80f77"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500], Loss: 3.7808\n",
            "Epoch [100/500], Loss: 5.1837\n",
            "Epoch [150/500], Loss: 3.2892\n",
            "Epoch [200/500], Loss: 3.6998\n",
            "Epoch [250/500], Loss: 3.9494\n",
            "Epoch [300/500], Loss: 5.0093\n",
            "Epoch [350/500], Loss: 4.0476\n",
            "Epoch [400/500], Loss: 3.1325\n",
            "Epoch [450/500], Loss: 4.0927\n",
            "Epoch [500/500], Loss: 3.4747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** O√π sont alors stock√©es les estimations des  $\\theta_k$ ? Les extraire du r√©seau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Acc√©der aux param√®tres de la couche lin√©aire\n",
        "linear_layer_params = mySimpleNet.fc.parameters()\n",
        "\n",
        "# Extraire les poids et le biais\n",
        "weights = next(iter(linear_layer_params)).detach().numpy()  # poids (Œ∏1, Œ∏2, Œ∏3)\n",
        "bias = next(iter(linear_layer_params)).detach().numpy()  # biais (Œ∏0)\n",
        "\n",
        "# Afficher les estimations des coefficients\n",
        "print(\"Poids (Œ∏1, Œ∏2, Œ∏3) :\", weights)\n",
        "print(\"Biais (Œ∏0) :\", bias)"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff75b215-9d04-46b4-d5aa-f8ab1f4955bd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poids (Œ∏1, Œ∏2, Œ∏3) : [[3.904183  3.8635218 7.58983  ]]\n",
            "Biais (Œ∏0) : [19.384892]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Charger les donn√©es de test\n",
        "test_inputs = test_set['inputs']  # Pr√©dicteurs du jeu de test\n",
        "test_targets = test_set['targets']  # Cibles r√©elles du jeu de test\n",
        "\n",
        "# D√©placer les donn√©es sur le m√™me appareil que le mod√®le\n",
        "test_inputs = torch.tensor(test_inputs, dtype=torch.float32)\n",
        "\n",
        "# G√©n√©rer les pr√©dictions √† l'aide du mod√®le\n",
        "mySimpleNet.eval()  # Mettre le mod√®le en mode √©valuation\n",
        "with torch.no_grad():  # Pas de calcul de gradients\n",
        "    predictions = mySimpleNet(test_inputs).numpy().flatten()  # Pr√©dictions aplaties\n",
        "\n",
        "# Calculer l'erreur quadratique moyenne (MSE) entre les pr√©dictions et les cibles\n",
        "mse_test = np.mean((predictions - test_targets) ** 2)\n",
        "\n",
        "# Affichage des r√©sultats\n",
        "print(f\"Erreur quadratique moyenne (MSE) sur le jeu de test : {mse_test:.4f}\")\n",
        "\n",
        "# Comparaison avec les r√©sultats obtenus dans la question 1\n",
        "# Si vous avez les pr√©dictions ou coefficients pr√©c√©dents, ins√©rez-les ici pour comparaison\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ILBESLbgxpV",
        "outputId": "8632b50e-077e-461e-df06-cb1dd5816471"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur quadratique moyenne (MSE) sur le jeu de test : 3.8208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparaison des erreurs quadratiques moyennes (MSE) :\n",
        "\n",
        "MSE sur le jeu de test avec le r√©seau de neurones : 3.8208\\\n",
        "MSE obtenu dans la question 1  :\n",
        "3.9825\n",
        "\n",
        "La MSE obtenue avec le r√©seau de neurones est l√©g√®rement inf√©rieure √† celle obtenue dans la question 1, ce qui montre que la deuxi√®me m√©thode est plus efficace mais la premi√®re est plus optimale en terme de temps."
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ r√©ceptif et pr√©diction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le r√©seau d√©fini dans la cellule suivante est utilis√© pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une s√©rie temporelle d'entr√©e et la valeur pr√©sente $y_t$ d'une s√©rie temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# S√©rie temporelle d'entr√©e (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# S√©rie temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbb10f4-e685-49ec-d598-1355f87de012"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de r√©seau de neurones s'agit-il ? Combien de param√®tres la couche self.Down1 compte-t-elle (√† faire √† la main) ?\n",
        "Combien de param√®tres le r√©seau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Type de r√©seau : Le r√©seau pr√©sent√© est un Fully Convolutional Network (FCN).\n",
        "# Nb de param√®tres dans self.Down1: (calcul \"√† la main\")\n",
        "# Nombre¬†de¬†param√®tres = 64√ó128√ó3 + 128 = 24704\n",
        "# Nb de param√®tres au total:\n",
        "# Calculer le nombre total de param√®tres dans le r√©seau\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Nombre total de param√®tres du r√©seau : {total_params}\")"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb947fc-e500-4a55-d163-4489feb8b86c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de param√®tres du r√©seau : 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels m√©canismes la taille du vecteur d'entr√©e est-elle r√©duite ? Comment est-elle restitu√©e dans la deuxi√®me partie du r√©seau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reponse:\n",
        "\n",
        "La taille du vecteur d'entr√©e est r√©duite √† traver la fonction Down_causal.\\\n",
        "Elle restuti√© √† l'aide de Up_causal"
      ],
      "metadata": {
        "id": "vjtuXnd-nWtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fsqjXc-3mL4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels m√©canismes le champ r√©ceptif est-il augment√© ? Pr√©ciser par un calcul la taille du champ r√©ceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le champ r√©ceptif est augment√© par l'empilement de couches de convolution et la dilatation, permettant aux neurones de capter des informations sur une zone plus large de l'entr√©e. La couche self.inc, compos√©e de deux convolutions causales avec un kernel_size de 3 et une dilation de 1, a un champ r√©ceptif en sortie de 5, calcul√© comme suit: 1 (initial) + 3 (1√®re convolution) + (3-1) (2√®me convolution) = 5."
      ],
      "metadata": {
        "id": "IIiAzXHKpYCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, d√©terminer empiriquement la taille du champ r√©ceptif associ√© √† la composante $y_{5000}$ du vecteur de sortie. (Indice: consid√©rer les sorties associ√©es √† deux inputs qui ne diff√®rent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Cr√©er deux entr√©es qui ne diff√®rent que par une composante\n",
        "input1 = torch.zeros(1, 1, 10000)\n",
        "input2 = input1.clone()\n",
        "input2[0, 0, 5000] = 1  # Modifier une seule composante\n",
        "\n",
        "# Obtenir les sorties du mod√®le pour les deux entr√©es\n",
        "model.eval()  # Mettre le mod√®le en mode √©valuation\n",
        "with torch.no_grad():\n",
        "    output1 = model(input1)\n",
        "    output2 = model(input2)\n",
        "\n",
        "# Calculer la diff√©rence entre les sorties\n",
        "diff = output2 - output1\n",
        "\n",
        "# Trouver les indices des composantes non nulles dans la diff√©rence\n",
        "receptive_field_indices = torch.nonzero(diff.squeeze())\n",
        "\n",
        "# Calculer la taille du champ r√©ceptif\n",
        "receptive_field_size = receptive_field_indices[-1] - receptive_field_indices[0] + 1\n",
        "\n",
        "# Afficher la taille du champ r√©ceptif\n",
        "print(\"Taille du champ r√©ceptif :\", receptive_field_size.item())"
      ],
      "metadata": {
        "id": "69WMWCSZAg5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee544458-aa01-411e-c724-ce5c6e67dc7c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du champ r√©ceptif : 305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ d√©pend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de mani√®re empirique puis pr√©ciser la partie du code de Double_conv_causal qui garantit cette propri√©t√© de \"causalit√©\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non, $ùë¶_{5000}$ ne d√©pend pas des composantes $x_{t, \\space t > 5000}$. Le champ r√©ceptif empirique est de 305, donc $ùë¶_{5000}$ d√©pend uniquement de ùë•ùë° pour 4696‚â§ùë°‚â§5000 . Le padding causal dans Double_conv_causal, garantit la causalit√© en ajoutant des z√©ros uniquement du c√¥t√© gauche de l'entr√©e, emp√™chant la convolution de \"voir\" les valeurs futures."
      ],
      "metadata": {
        "id": "59BxF4kzq6nB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeooRYE-ATGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article r√©cent](https://https://arxiv.org/abs/2403.14144) revient sur les progr√®s en mati√®re de learning to rank. En voil√† un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "texte en italique\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?token=GHSAT0AAAAAAC427DACOPGNDNN6UDOLVLLAZ4BB2JQ\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pQaa6Uxus2Mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R√©ponse:\n",
        "\n",
        "Les \"positive samples\" d√©signent les √©chantillons qui sont jug√©s pertinents pour une requ√™te donn√©e, tandis que les \"negative samples\" d√©signent ceux qui sont jug√©s non pertinents."
      ],
      "metadata": {
        "id": "Imi5vwBvszaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'o√π proviennent les $z_i$ ? Que repr√©sentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R√©ponse:\n",
        "Les zi\n",
        "  proviennent du mod√®le de pr√©diction. Ils repr√©sentent les scores pr√©vus par le mod√®le pour chaque √©chantillon.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mHXx2ukQs_Gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle √† ce que, apr√®s apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'expression $\\mathcal{L}_{RankNet}$ inclut une fonction log-sigmo√Øde (\n",
        "ùúé(ùëßùëñ ‚àí ùëßùëó)\n",
        " )) qui maximise la probabilit√© que le score\n",
        "zi  soit sup√©rieur √† zj\n",
        "  pour un √©chantillon positif (yij = 1) et minimise cette probabilit√© pour un √©chantillon n√©gatif (yij = 0). Par cons√©quent, l'optimisation de cette fonction de perte pousse le mod√®le √† produire des scores (zi > zj)\n",
        "  pour des paires positives/n√©gatives, ce qui garantit que les √©chantillons positifs obtiennent des scores sup√©rieurs aux √©chantillons n√©gatifs."
      ],
      "metadata": {
        "id": "qH6x0TXTt4T1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les r√©seaux de neurones exploit√©s et la modalit√© suivant laquelle ils sont entra√Æn√©s ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}